<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.2">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"syxdevcode.github.com","root":"/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":true,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="转载： 云网络丢包故障定位全景指南 硬件网卡丢包Ring Buffer溢出 如图所示，物理介质上的数据帧到达后首先由NIC（网络适配器）读取，写入设备内部缓冲区 Ring Buffer中，再由中断处理程序触发 Softirq 从中消费，Ring Buffer 的大小因网卡设备而异。当网络数据包到达（生产）的速率快于内核处理（消费）的速率时，Ring Buffer 很快会被填满，新来的数据包将被丢弃">
<meta property="og:type" content="article">
<meta property="og:title" content="Linux下网络丢包故障定位">
<meta property="og:url" content="https://syxdevcode.github.com/2021/03/01/Linux%E4%B8%8B%E7%BD%91%E7%BB%9C%E4%B8%A2%E5%8C%85%E6%95%85%E9%9A%9C%E5%AE%9A%E4%BD%8D/index.html">
<meta property="og:site_name" content="syxdevcode博客">
<meta property="og:description" content="转载： 云网络丢包故障定位全景指南 硬件网卡丢包Ring Buffer溢出 如图所示，物理介质上的数据帧到达后首先由NIC（网络适配器）读取，写入设备内部缓冲区 Ring Buffer中，再由中断处理程序触发 Softirq 从中消费，Ring Buffer 的大小因网卡设备而异。当网络数据包到达（生产）的速率快于内核处理（消费）的速率时，Ring Buffer 很快会被填满，新来的数据包将被丢弃">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://syxdevcode.github.com/img/640.png">
<meta property="og:image" content="https://syxdevcode.github.com/img/640-1.png">
<meta property="og:image" content="https://syxdevcode.github.com/img/640.webp">
<meta property="og:image" content="https://syxdevcode.github.com/img/640-2.png">
<meta property="og:image" content="https://syxdevcode.github.com/img/640-3.png">
<meta property="og:image" content="https://syxdevcode.github.com/img/640-4.png">
<meta property="og:image" content="https://syxdevcode.github.com/img/640-1.webp">
<meta property="og:image" content="https://syxdevcode.github.com/img/640-2.webp">
<meta property="og:image" content="https://syxdevcode.github.com/img/640-3.webp">
<meta property="og:image" content="https://syxdevcode.github.com/img/640-5.png">
<meta property="og:image" content="https://syxdevcode.github.com/img/640-6.png">
<meta property="og:image" content="https://syxdevcode.github.com/img/640-7.png">
<meta property="og:image" content="https://syxdevcode.github.com/img/640-8.png">
<meta property="og:image" content="https://syxdevcode.github.com/img/640-9.png">
<meta property="og:image" content="https://syxdevcode.github.com/img/640-10.png">
<meta property="og:image" content="https://syxdevcode.github.com/img/640-11.png">
<meta property="og:image" content="https://syxdevcode.github.com/img/640-14.png">
<meta property="og:image" content="https://syxdevcode.github.com/img/640-12.png">
<meta property="og:image" content="https://syxdevcode.github.com/img/640-4.webp">
<meta property="og:image" content="https://syxdevcode.github.com/img/640-5.webp">
<meta property="og:image" content="https://syxdevcode.github.com/img/640-13.png">
<meta property="og:image" content="https://syxdevcode.github.com/img/640-6.webp">
<meta property="og:image" content="https://syxdevcode.github.com/img/640-7.webp">
<meta property="og:image" content="https://syxdevcode.github.com/img/640-15.png">
<meta property="og:image" content="https://syxdevcode.github.com/img/640-16.png">
<meta property="og:image" content="https://syxdevcode.github.com/img/640-17.png">
<meta property="og:image" content="https://syxdevcode.github.com/img/640-18.png">
<meta property="og:image" content="https://syxdevcode.github.com/img/640-19.png">
<meta property="og:image" content="https://syxdevcode.github.com/img/640-20.png">
<meta property="og:image" content="https://syxdevcode.github.com/img/640-8.webp">
<meta property="og:image" content="https://syxdevcode.github.com/img/640-21.png">
<meta property="og:image" content="https://syxdevcode.github.com/img/640-9.webp">
<meta property="og:image" content="https://syxdevcode.github.com/img/640-10.webp">
<meta property="og:image" content="https://syxdevcode.github.com/img/640-11.webp">
<meta property="og:image" content="https://syxdevcode.github.com/img/640-12.webp">
<meta property="og:image" content="https://syxdevcode.github.com/img/640-13.webp">
<meta property="og:image" content="https://syxdevcode.github.com/img/640-22.png">
<meta property="og:image" content="https://syxdevcode.github.com/img/640-23.png">
<meta property="og:image" content="https://syxdevcode.github.com/img/640-24.png">
<meta property="og:image" content="https://syxdevcode.github.com/img/640-25.png">
<meta property="og:image" content="https://syxdevcode.github.com/img/640-26.png">
<meta property="article:published_time" content="2021-03-01T11:34:08.000Z">
<meta property="article:modified_time" content="2022-09-28T05:26:22.784Z">
<meta property="article:author" content="syxdevcode">
<meta property="article:tag" content="计算机基础">
<meta property="article:tag" content="网络基础">
<meta property="article:tag" content="TCP协议">
<meta property="article:tag" content="Linux网络">
<meta property="article:tag" content="IP网络">
<meta property="article:tag" content="Ethernet">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://syxdevcode.github.com/img/640.png">

<link rel="canonical" href="https://syxdevcode.github.com/2021/03/01/Linux%E4%B8%8B%E7%BD%91%E7%BB%9C%E4%B8%A2%E5%8C%85%E6%95%85%E9%9A%9C%E5%AE%9A%E4%BD%8D/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>Linux下网络丢包故障定位 | syxdevcode博客</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">syxdevcode博客</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="reading-progress-bar"></div>

  <a href="https://github.com/syxdevcode" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://syxdevcode.github.com/2021/03/01/Linux%E4%B8%8B%E7%BD%91%E7%BB%9C%E4%B8%A2%E5%8C%85%E6%95%85%E9%9A%9C%E5%AE%9A%E4%BD%8D/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="syxdevcode">
      <meta itemprop="description" content="syxdevcode的个人博客">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="syxdevcode博客">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Linux下网络丢包故障定位
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-03-01 11:34:08" itemprop="dateCreated datePublished" datetime="2021-03-01T11:34:08+00:00">2021-03-01</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2022-09-28 05:26:22" itemprop="dateModified" datetime="2022-09-28T05:26:22+00:00">2022-09-28</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Ethernet/" itemprop="url" rel="index"><span itemprop="name">Ethernet</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>18k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>16 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>转载：</p>
<p><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/DP0F3Q7TnOixxWZ9-_KA4A">云网络丢包故障定位全景指南</a></p>
<h2 id="硬件网卡丢包"><a href="#硬件网卡丢包" class="headerlink" title="硬件网卡丢包"></a>硬件网卡丢包</h2><h3 id="Ring-Buffer溢出"><a href="#Ring-Buffer溢出" class="headerlink" title="Ring Buffer溢出"></a>Ring Buffer溢出</h3><p><img src="/img/640.png" alt="640.png"></p>
<p>如图所示，物理介质上的数据帧到达后首先由NIC（网络适配器）读取，写入设备内部缓冲区 Ring Buffer中，再由中断处理程序触发 Softirq 从中消费，Ring Buffer 的大小因网卡设备而异。当网络数据包到达（生产）的速率快于内核处理（消费）的速率时，Ring Buffer 很快会被填满，新来的数据包将被丢弃；</p>
<ol>
<li>查看：</li>
</ol>
<p>通过 ethtool 或 /proc/net/dev 可以查看因 Ring Buffer 满而丢弃的包统计，在统计项中,以fifo标识：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ ethtool -S eth0|grep rx_fifo</span><br><span class="line">rx_fifo_errors: 0</span><br><span class="line"></span><br><span class="line">$ <span class="built_in">cat</span> /proc/net/dev</span><br><span class="line">Inter-|   Receive                                                |  Transmit</span><br><span class="line"> face |bytes    packets errs drop fifo frame compressed multicast|bytes    packets errs drop fifo colls carrier compressed</span><br><span class="line">  eth0: 3623955871 24771436    0    0    0     0          0         0 1876696873 11380645    0    0    0     0       0          0</span><br><span class="line">    lo: 5324832  115757    0    0    0     0          0         0  5324832  115757    0    0    0     0       0          0</span><br></pre></td></tr></table></figure>

<ol start="2">
<li>查看eth0网卡Ring Buffer最大值和当前设置</li>
</ol>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ethtool -g eth0</span><br></pre></td></tr></table></figure>

<ol start="3">
<li>解决方案：修改网卡eth0接收与发送硬件缓存区大小</li>
</ol>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ethtool -G eth0 rx 4096 tx 4096</span><br></pre></td></tr></table></figure>

<h3 id="网卡端口协商丢包"><a href="#网卡端口协商丢包" class="headerlink" title="网卡端口协商丢包"></a>网卡端口协商丢包</h3><ol>
<li>查看网卡丢包统计：<code>ethtool -S eth0</code></li>
</ol>
<p><img src="/img/640-1.png" alt="640-1.png"></p>
<ol start="2">
<li>查看网卡配置状态：<code>ethtool eth0</code></li>
</ol>
<p><img src="/img/640.webp" alt="640.webp"></p>
<p>主要查看网卡和上游网络设备协商速率和模式是否符合预期；</p>
<p>解决方案：</p>
<ol>
<li>重新自协商：<code>ethtool -r eth0</code>;</li>
<li>如果上游不支持自协商，可以强制设置端口速率：</li>
</ol>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ethtool -s eth0 speed 1000 duplex full autoneg off</span><br></pre></td></tr></table></figure>

<h3 id="网卡流控丢包"><a href="#网卡流控丢包" class="headerlink" title="网卡流控丢包"></a>网卡流控丢包</h3><ol>
<li>查看流控统计：</li>
</ol>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ethtool -S eth1 | grep control</span><br></pre></td></tr></table></figure>

<p><img src="/img/640-2.png" alt="640-2.png"></p>
<p>rx_flow_control_xon 是在网卡的 RX Buffer 满或其他网卡内部的资源受限时，给交换机端口发送的开启流控的pause帧计数。对应的，tx_flow_control_xoff 是在资源可用之后发送的关闭流控的pause帧计数。</p>
<ol start="2">
<li>查看网络流控配置：<code>ethtool -a eth1</code></li>
</ol>
<p><img src="/img/640-3.png" alt="640-3.png"></p>
<ol start="3">
<li>解决方案：关闭网卡流控</li>
</ol>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ethtool -A ethx autoneg off  <span class="comment"># 自协商关闭</span></span><br><span class="line">ethtool -A ethx tx off  <span class="comment"># 发送模块关闭</span></span><br><span class="line">ethtool -A ethx rx off <span class="comment"># 接收模块关闭</span></span><br></pre></td></tr></table></figure>

<h3 id="报文mac地址丢包"><a href="#报文mac地址丢包" class="headerlink" title="报文mac地址丢包"></a>报文mac地址丢包</h3><p>一般计算机网卡都工作在非混杂模式下，此时网卡只接受来自网络端口的目的地址指向自己的数据，如果报文的目的mac地址不是对端的接口的mac地址，一般都会丢包，一般这种情况很有可能是源端设置静态arp表项或者动态学习的arp表项没有及时更新，但目的端mac地址已发生变化（换了网卡），没有更新通知到源端（比如更新报文被丢失，中间交换机异常等情况）；</p>
<p>查看： </p>
<ol>
<li>目的端抓包，tcpdump可以开启混杂模式，可以抓到对应的报文，然后查看mac地址；</li>
<li>源端查看arp表或者抓包（上一跳设备），看发送的mac地址是否和下一跳目的端的mac地址一致；</li>
</ol>
<p>解决方案：</p>
<ol>
<li>刷新arp表然后发包触发arp重新学习（可能影响其他报文，增加延时，需要小心操作）；</li>
<li>可以在源端手动设置正确的静态的arp表项；</li>
</ol>
<h3 id="其他网卡异常丢包"><a href="#其他网卡异常丢包" class="headerlink" title="其他网卡异常丢包"></a>其他网卡异常丢包</h3><p>这类异常比少见，但如果都不是上面哪些情况，但网卡统计里面仍然有丢包计数，可以试着排查一下：</p>
<h4 id="网卡firmware版本"><a href="#网卡firmware版本" class="headerlink" title="网卡firmware版本:"></a>网卡firmware版本:</h4><p>排查一下网卡phy芯片firmware是不是有bug，安装的版本是不是符合预期，查看 ethtool -i eth1:</p>
<p>eth1:</p>
<p><img src="/img/640-4.png" alt="640-4.png"></p>
<p>和厂家提case询问是不是已知问题，有没有新版本等；</p>
<h4 id="网线接触不良："><a href="#网线接触不良：" class="headerlink" title="网线接触不良："></a>网线接触不良：</h4><p>如果网卡统计里面存在crc error 计数增长，很可能是网线接触不良，可以通知网管排查一下：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ethtool -S eth0</span><br></pre></td></tr></table></figure>

<p><img src="/img/640-1.webp" alt="640-1.webp"></p>
<p>解决方案：一般试着重新插拔一下网线，或者换一根网线，排查插口是否符合端口规格等;</p>
<h4 id="报文长度丢包"><a href="#报文长度丢包" class="headerlink" title="报文长度丢包"></a>报文长度丢包</h4><p>网卡有接收正确报文长度范围，一般正常以太网报文长度范围：64-1518，发送端正常情况会填充或者分片来适配，偶尔会发生一些异常情况导致发送报文不正常丢包；</p>
<p>查看：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ethtool -S eth1|grep length_errors</span><br></pre></td></tr></table></figure>

<p><img src="/img/640-2.webp" alt="640-2.webp"></p>
<p>解决方案：</p>
<p>1  调整接口MTU配置，是否开启支持以太网巨帧；</p>
<p>2  发送端开启PATH MTU进行合理分片；</p>
<p>简单总结一下网卡丢包：</p>
<p><img src="/img/640-3.webp" alt="640-3.webp"></p>
<h2 id="网卡驱动丢包"><a href="#网卡驱动丢包" class="headerlink" title="网卡驱动丢包"></a>网卡驱动丢包</h2><p>查看：<code>ifconfig eth1/eth0</code> 等接口</p>
<p><img src="/img/640-5.png" alt="640-5.png"></p>
<ol>
<li>RX errors: 表示总的收包的错误数量，还包括too-long-frames错误，Ring Buffer 溢出错误，crc 校验错误，帧同步错误，fifo overruns 以及 missed pkg 等等。</li>
<li>RX dropped: 表示数据包已经进入了 Ring Buffer，但是由于内存不够等系统原因，导致在拷贝到内存的过程中被丢弃。</li>
<li>RX overruns: 表示了 fifo 的 overruns，这是由于 Ring Buffer(aka Driver Queue) 传输的 IO 大于 kernel 能够处理的 IO 导致的，而 Ring Buffer 则是指在发起 IRQ 请求之前的那块 buffer。很明显，overruns 的增大意味着数据包没到 Ring Buffer 就被网卡物理层给丢弃了，而 CPU 无法即使的处理中断是造成 Ring Buffer 满的原因之一，上面那台有问题的机器就是因为 interruprs 分布的不均匀(都压在 core0)，没有做 affinity 而造成的丢包。</li>
<li>RX frame: 表示 misaligned 的 frames。</li>
<li>对于 TX 的来说，出现上述 counter 增大的原因主要包括 aborted transmission, errors due to carrirer, fifo error, heartbeat erros 以及 windown error，而 collisions 则表示由于 CSMA/CD 造成的传输中断。</li>
</ol>
<h3 id="驱动溢出丢包"><a href="#驱动溢出丢包" class="headerlink" title="驱动溢出丢包"></a>驱动溢出丢包</h3><p>netdev_max_backlog 是内核从NIC(网卡(Network Interface Card，简称NIC)，也称网络适配器)收到包后，交由协议栈（如IP、TCP）处理之前的缓冲队列。每个CPU核都有一个backlog队列，与 Ring Buffer 同理，当接收包的速率大于内核协议栈处理的速率时，CPU的backlog队列不断增长，当达到设定的 netdev_max_backlog 值时，数据包将被丢弃。</p>
<p>查看:</p>
<p>通过查看 <code>/proc/net/softnet_stat</code> 可以确定是否发生了 <code>netdev backlog</code> 队列溢出：</p>
<p><img src="/img/640-6.png" alt="640-6.png"></p>
<p>其中：每一行代表每个CPU核的状态统计，从CPU0依次往下；每一列代表一个CPU核的各项统计：第一列代表中断处理程序收到的包总数；第二列即代表由于 netdev_max_backlog 队列溢出而被丢弃的包总数。从上面的输出可以看出，这台服务器统计中，并没有因为 netdev_max_backlog 导致的丢包。</p>
<p>解决方案：</p>
<p>netdev_max_backlog 的默认值是 1000，在高速链路上，可能会出现上述第二统计不为0的情况，可以通过修改内核参数 net.core.netdev_max_backlog 来解决：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sysctl -w net.core.netdev_max_backlog=2000</span><br></pre></td></tr></table></figure>

<h3 id="单核负载高导致丢包"><a href="#单核负载高导致丢包" class="headerlink" title="单核负载高导致丢包"></a>单核负载高导致丢包</h3><p>单核CPU软中断占有高, 导致应用没有机会收发或者收包比较慢，即使调整 netdev_max_backlog 队列大小仍然会一段时间后丢包，处理速度跟不上网卡接收的速度;</p>
<p>查看：<code>mpstat -P ALL 1</code></p>
<p><img src="/img/640-7.png" alt="640-7.png"></p>
<p>单核软中断占有100%，导致应用没有机会收发或者收包比较慢而丢包；</p>
<p><strong>解决方案：</strong></p>
<ol>
<li>调整网卡RSS队列配置：</li>
</ol>
<p>查看：<code>ethtool -x ethx</code>；<br>调整：<code>ethtool -X ethx xxxx</code>；</p>
<ol start="2">
<li>看一下网卡中断配置是否均衡 <code>cat /proc/interrupts</code></li>
</ol>
<p>调整：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1） irqbalance 调整；</span></span><br><span class="line"><span class="comment"># 查看当前运行情况</span></span><br><span class="line">service irqbalance status</span><br><span class="line"><span class="comment"># 终止服务</span></span><br><span class="line">service irqbalance stop</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2） 中断绑CPU核 </span></span><br><span class="line"><span class="built_in">echo</span> mask &gt; /proc/irq/xxx/smp_affinity</span><br></pre></td></tr></table></figure>

<ol start="3">
<li>根据CPU和网卡队列个数调整网卡多队列和RPS配置</li>
</ol>
<p>-CPU大于网卡队列个数：</p>
<p>查看网卡队列 <code>ethtool -x ethx</code>；</p>
<p>协议栈开启RPS并设置RPS；</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> <span class="variable">$mask</span>（CPU配置）&gt; /sys/class/net/<span class="variable">$eth</span>/queues/rx-<span class="variable">$i</span>/rps_cpus</span><br><span class="line"><span class="built_in">echo</span> 4096（网卡buff）&gt; /sys/class/net/<span class="variable">$eth</span>/queues/rx-<span class="variable">$i</span>/rps_flow_cnt</span><br></pre></td></tr></table></figure>

<p>2）CPU小于网卡队列个数，绑中断就可以，可以试着关闭RPS看一下效果：</p>
<p><code>echo 0 &gt; /sys/class/net/&lt;dev&gt;/queues/rx-&lt;n&gt;/rps_cpus</code></p>
<p>4.numa CPU 调整，对齐网卡位置，可以提高内核处理速度，从而给更多CPU给应用收包，减缓丢包概率；</p>
<p>查看网卡numa位置：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ethtool -i eth1|grep bus-info</span><br><span class="line">lspci -s bus-info -vv|grep node</span><br></pre></td></tr></table></figure>

<p>上面中断和RPS设置里面mask需要重新按numa CPU分配重新设置;</p>
<p>5.可以试着开启中断聚合（看网卡是否支持）</p>
<p>查看 : </p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">ethtool -c ethx</span><br><span class="line">Coalesce parameters <span class="keyword">for</span> eth1:</span><br><span class="line">Adaptive RX: on  TX: on</span><br><span class="line">stats-block-usecs: 0</span><br><span class="line">sample-interval: 0</span><br><span class="line">pkt-rate-low: 0</span><br><span class="line">pkt-rate-high: 0</span><br><span class="line"></span><br><span class="line">rx-usecs: 25</span><br><span class="line">rx-frames: 0</span><br><span class="line">rx-usecs-irq: 0</span><br><span class="line">rx-frames-irq: 256</span><br><span class="line"></span><br><span class="line">tx-usecs: 25</span><br><span class="line">tx-frames: 0</span><br><span class="line">tx-usecs-irq: 0</span><br><span class="line">tx-frames-irq: 256</span><br><span class="line"></span><br><span class="line">rx-usecs-low: 0</span><br><span class="line">rx-frame-low: 0</span><br><span class="line">tx-usecs-low: 0</span><br><span class="line">tx-frame-low: 0</span><br><span class="line"></span><br><span class="line">rx-usecs-high: 0</span><br><span class="line">rx-frame-high: 0</span><br><span class="line">tx-usecs-high: 0</span><br><span class="line">tx-frame-high: 0</span><br></pre></td></tr></table></figure>

<p>调整：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ethtool -C ethx adaptive-rx on</span><br></pre></td></tr></table></figure>

<p>简单总结一下网卡驱动丢包处理：</p>
<p><img src="/img/640-8.png" alt="640-8.png"></p>
<h2 id="内核协议栈丢包"><a href="#内核协议栈丢包" class="headerlink" title="内核协议栈丢包"></a>内核协议栈丢包</h2><h3 id="以太网链路层丢包"><a href="#以太网链路层丢包" class="headerlink" title="以太网链路层丢包"></a>以太网链路层丢包</h3><h4 id="neighbor-系统arp丢包"><a href="#neighbor-系统arp丢包" class="headerlink" title="neighbor 系统arp丢包"></a>neighbor 系统arp丢包</h4><h5 id="arp-ignore-配置丢包"><a href="#arp-ignore-配置丢包" class="headerlink" title="arp_ignore 配置丢包"></a>arp_ignore 配置丢包</h5><p>arp_ignore 参数的作用是控制系统在收到外部的arp请求时，是否要返回arp响应。arp_ignore 参数常用的取值主要有 0，1，2，3~8 较少用到；</p>
<p>查看：<code>sysctl -a|grep arp_ignore</code></p>
<p><img src="/img/640-9.png" alt="640-9.png"></p>
<p>解决方案：根据实际场景设置对应值；</p>
<ul>
<li>0：响应任意网卡上接收到的对本机IP地址的arp请求（包括环回网卡上的地址），而不管该目的IP是否在接收网卡上。</li>
<li>1：只响应目的IP地址为接收网卡上的本地地址的arp请求。</li>
<li>2：只响应目的IP地址为接收网卡上的本地地址的arp请求，并且arp请求的源IP必须和接收网卡同网段。</li>
<li>3：如果ARP请求数据包所请求的IP地址对应的本地地址其作用域（scope）为主机（host），则不回应ARP响应数据包，如果作用域为全局（global）或链路（link），则回应ARP响应数据包。</li>
</ul>
<p><img src="/img/640-10.png" alt="640-10.png"></p>
<p><img src="/img/640-11.png" alt="640-11.png"></p>
<h5 id="arp-filter配置丢包"><a href="#arp-filter配置丢包" class="headerlink" title="arp_filter配置丢包"></a>arp_filter配置丢包</h5><p>在多接口系统里面（比如腾讯云的弹性网卡场景），这些接口都可以回应arp请求，导致对端有可能学到不同的mac地址，后续报文发送可能由于mac地址和接收报文接口mac地址不一样而导致丢包，arp_filter主要是用来适配这种场景；</p>
<p>查看：<code>sysctl -a | grep arp_filter</code></p>
<p><img src="/img/640-14.png" alt="640-14.png"></p>
<p>解决方案： </p>
<p>根据实际场景设置对应的值，一般默认是关掉此过滤规则，特殊情况可以打开；<br>0：默认值，表示回应arp请求的时候不检查接口情况；<br>1：表示回应arp请求时会检查接口是否和接收请求接口一致，不一致就不回应；</p>
<h4 id="arp表满导致丢包"><a href="#arp表满导致丢包" class="headerlink" title="arp表满导致丢包"></a>arp表满导致丢包</h4><p>比如下面这种情况，由于突发arp表项很多 超过协议栈默认配置，发送报文的时候部分arp创建失败，导致发送失败，从而丢包：</p>
<p><img src="/img/640-12.png" alt="640-12.png"></p>
<p>查看：</p>
<ol>
<li>查看arp状态：<code>cat /proc/net/stat/arp_cache</code> ，table_fulls 统计：</li>
</ol>
<p><img src="/img/640-4.webp" alt="640-4.webp"></p>
<ol start="2">
<li>查看dmesg消息（内核打印）：</li>
</ol>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">dmesg|grep neighbour</span><br><span class="line">neighbour: arp_cache: neighbor table overflow!</span><br></pre></td></tr></table></figure>

<ol start="3">
<li>查看当前arp表大小：<code>ip n|wc -l</code></li>
</ol>
<p>查看系统配额：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">sysctl -a |grep net.ipv4.neigh.default.gc_thresh</span><br><span class="line">gc_thresh1：存在于ARP高速缓存中的最少层数，如果少于这个数，垃圾收集器将不会运行。缺省值是128。</span><br><span class="line"></span><br><span class="line">gc_thresh2 ：保存在 ARP 高速缓存中的最多的记录软限制。垃圾收集器在开始收集前，允许记录数超过这个数字 5 秒。缺省值是 512。</span><br><span class="line">gc_thresh3 ：保存在 ARP 高速缓存中的最多记录的硬限制，一旦高速缓存中的数目高于此，垃圾收集器将马上运行。缺省值是1024。</span><br></pre></td></tr></table></figure>

<p>一般在内存足够情况下，可以认为 gc_thresh3 值是arp 表总大小；</p>
<p><img src="/img/640-5.webp" alt="640-5.webp"></p>
<p>解决方案：根据实际arp最大值情况（比如访问其他子机最大个数），调整arp表大小</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sudo sysctl -w net.ipv4.neigh.default.gc_thresh1=1024</span><br><span class="line">sudo sysctl -w net.ipv4.neigh.default.gc_thresh2=2048</span><br><span class="line">sudo sysctl -w net.ipv4.neigh.default.gc_thresh3=4096</span><br><span class="line">sudo sysctl  -p</span><br></pre></td></tr></table></figure>

<h4 id="arp请求缓存队列溢出丢包"><a href="#arp请求缓存队列溢出丢包" class="headerlink" title="arp请求缓存队列溢出丢包"></a>arp请求缓存队列溢出丢包</h4><p>查看：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># unresolved_discards是否有新增计数</span></span><br><span class="line"><span class="built_in">cat</span> /proc/net/stat/arp_cache</span><br></pre></td></tr></table></figure>

<p>解决方案：根据客户需求调整缓存队列大小 unres_qlen_bytes </p>
<p><code>sysctl -a | grep unres_qlen_bytes</code></p>
<p><img src="/img/640-13.png" alt="640-13.png"></p>
<h3 id="网络IP层丢包"><a href="#网络IP层丢包" class="headerlink" title="网络IP层丢包"></a>网络IP层丢包</h3><h4 id="接口ip地址配置丢包"><a href="#接口ip地址配置丢包" class="headerlink" title="接口ip地址配置丢包"></a>接口ip地址配置丢包</h4><ol>
<li>本机服务不通，检查lo接口有没有配置地址是 127.0.0.1；</li>
<li>本机接收失败， 查看local路由表：<code>ip r show table local | grep 子机ip地址</code>；这种丢包一般会出现在多IP场景，子机底层配置多ip失败，导致对应ip收不到包而丢包；</li>
</ol>
<p><img src="/img/640-6.webp" alt="640-6.webp"></p>
<p>解决方案：</p>
<ol>
<li>配置正确接口ip地址；比如 <code>ip a add 1.1.1.1 dev eth0</code></li>
<li>如果发现接口有地址还丢包，可能是local路由表没有对应条目，紧急情况下，可以用手工补上：<br>比如 <code>ip r add local</code> 本机ip地址 <code>dev eth0 table local</code> ；</li>
</ol>
<h4 id="路由丢包"><a href="#路由丢包" class="headerlink" title="路由丢包"></a>路由丢包</h4><h5 id="路由配置丢包"><a href="#路由配置丢包" class="headerlink" title="路由配置丢包"></a>路由配置丢包</h5><p>查看：</p>
<ol>
<li>查看配置 路由是否设置正确（是否可达），是否配置策略路由（在弹性网卡场景会出现此配置）<code>ip rule</code>：</li>
</ol>
<p><img src="/img/640-7.webp" alt="640-7.webp"></p>
<p>然后找到对应路由表。查看路由表：</p>
<p><img src="/img/640-15.png" alt="640-15.png"></p>
<p>或者直接用 ip r get x.x.x.x，让系统帮你查找是否存在可达路由，接口是否符合预期；</p>
<ol start="2">
<li>查看系统统计信息：  </li>
</ol>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">netstat -s|grep <span class="string">&quot;dropped because of missing route&quot;</span></span><br></pre></td></tr></table></figure>

<p>解决方案：重新配置正确的路由；</p>
<h5 id="反向路由过滤丢包"><a href="#反向路由过滤丢包" class="headerlink" title="反向路由过滤丢包"></a>反向路由过滤丢包</h5><p>反向路由过滤机制是Linux通过反向路由查询，检查收到的数据包源IP是否可路由（Loose mode）、是否最佳路由（Strict mode），如果没有通过验证，则丢弃数据包，设计的目的是防范IP地址欺骗攻击。</p>
<p>查看：</p>
<p>rp_filter 提供三种模式供配置：</p>
<ul>
<li>0 - 不验证</li>
<li>1 - RFC3704定义的严格模式：对每个收到的数据包，查询反向路由，如果数据包入口和反向路由出口不一致，则不通过</li>
<li>2 - RFC3704定义的松散模式：对每个收到的数据包，查询反向路由，如果任何接口都不可达，则不通过</li>
</ul>
<p>查看当前 rp_filter 策略配置：</p>
<p><code>cat /proc/sys/net/ipv4/conf/eth0/rp_filter</code></p>
<p>如果这里设置为1，就需要查看主机的网络环境和路由策略是否可能会导致客户端的入包无法通过反向路由验证了。</p>
<p>从原理来看这个机制工作在网络层，因此，如果客户端能够Ping通服务器，就能够排除这个因素了。</p>
<p>解决方案：</p>
<p>根据实际网络环境将rp_filter设置为 0 或 2：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sysctl -w net.ipv4.conf.all.rp_filter=2</span><br><span class="line"><span class="comment"># 或</span></span><br><span class="line">sysctl -w net.ipv4.conf.eth0.rp_filter=2</span><br></pre></td></tr></table></figure>

<h4 id="防火墙丢包"><a href="#防火墙丢包" class="headerlink" title="防火墙丢包"></a>防火墙丢包</h4><h5 id="客户设置规则导致丢包"><a href="#客户设置规则导致丢包" class="headerlink" title="客户设置规则导致丢包"></a>客户设置规则导致丢包</h5><p>查看：</p>
<p><code>iptables -nvL |grep DROP</code></p>
<p>解决方案：修改防火墙规则；</p>
<h4 id="连接跟踪导致丢包"><a href="#连接跟踪导致丢包" class="headerlink" title="连接跟踪导致丢包"></a>连接跟踪导致丢包</h4><p><strong>连接跟踪表溢出丢包</strong></p>
<p>kernel 用 ip_conntrack 模块来记录 iptables 网络包的状态，并把每条记录保存到 table 里（这个 table 在内存里，可以通过 <code>/proc/net/ip_conntrack</code> 查看当前已经记录的总数），如果网络状况繁忙，比如高连接，高并发连接等会导致逐步占用这个 table 可用空间，一般这个 table 很大不容易占满并且可以自己清理，table 的记录会一直呆在 table 里占用空间直到源 IP 发一个 RST 包，但是如果出现被攻击、错误的网络配置、有问题的路由/路由器、有问题的网卡等情况的时候，就会导致源 IP 发的这个 RST 包收不到，这样就积累在 table 里，越积累越多直到占满。无论，哪种情况导致table变满，满了以后就会丢包，出现外部无法连接服务器的情况。内核会报如下错误信息：<code>kernel: ip_conntrack: table full, dropping packet</code>；</p>
<p>查看当前连接跟踪数 :</p>
<p><code>cat /proc/sys/net/netfilter/nf_conntrack_max</code></p>
<p>解决方案：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 增大跟踪的最大条数</span></span><br><span class="line">net.netfilter.nf_conntrack_max  = 3276800</span><br><span class="line"></span><br><span class="line"><span class="comment"># 减少跟踪连接的最大有效时间</span></span><br><span class="line">net.netfilter.nf_conntrack_tcp_timeout_established = 1200</span><br><span class="line">net.netfilter.nf_conntrack_udp_timeout_stream = 180</span><br><span class="line">net.netfilter.nf_conntrack_icmp_timeout = 30</span><br></pre></td></tr></table></figure>

<h4 id="ct创建冲突失导致丢包"><a href="#ct创建冲突失导致丢包" class="headerlink" title="ct创建冲突失导致丢包"></a>ct创建冲突失导致丢包</h4><p>查看：当前连接跟踪统计：<code>cat /proc/net/stat/nf_conntrack</code>，可以查各种ct异常丢包统计</p>
<p><img src="/img/640-16.png" alt="640-16.png"></p>
<p>解决方案：内核热补丁修复或者更新内核版本（合入补丁修改）；</p>
<h3 id="传输层UDP-TCP丢包"><a href="#传输层UDP-TCP丢包" class="headerlink" title="传输层UDP/TCP丢包"></a>传输层UDP/TCP丢包</h3><h4 id="tcp-连接跟踪安全检查丢包"><a href="#tcp-连接跟踪安全检查丢包" class="headerlink" title="tcp 连接跟踪安全检查丢包"></a>tcp 连接跟踪安全检查丢包</h4><p>丢包原因：由于连接没有断开，但服务端或者client之前出现过发包异常等情况（报文没有经过连接跟踪模块更新窗口计数），没有更新合法的 <code>window</code> 范围，导致后续报文安全检查被丢包；协议栈用 <code>nf_conntrack_tcp_be_liberal</code> 来控制这个选项：</p>
<ul>
<li>1：关闭，只有不在tcp窗口内的rst包被标志为无效；</li>
<li>0：开启;   所有不在tcp窗口中的包都被标志为无效；</li>
</ul>
<p>查看： </p>
<p>查看配置 ：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sysctl -a|grep nf_conntrack_tcp_be_liberal </span><br><span class="line">net.netfilter.nf_conntrack_tcp_be_liberal = 1</span><br></pre></td></tr></table></figure>

<p>查看log：<br>一般情况下 netfiler 模块默认没有加载log，需要手动加载;</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">modprobe ipt_LOG11</span><br><span class="line">sysctl -w net.netfilter.nf_log.2=ipt_LOG</span><br></pre></td></tr></table></figure>

<p>然后发包后在查看syslog；</p>
<p>解决方案：根据实际抓包分析情况判断是不是此机制导致的丢包，可以试着关闭试一下；</p>
<h4 id="分片重组丢包"><a href="#分片重组丢包" class="headerlink" title="分片重组丢包"></a>分片重组丢包</h4><p>情况总结：超时</p>
<p>查看：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">netstat -s|grep <span class="built_in">timeout</span></span><br><span class="line">601 fragments dropped after <span class="built_in">timeout</span></span><br></pre></td></tr></table></figure>

<p>解决方法：调整超时时间</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">net.ipv4.ipfrag_time = 30</span><br><span class="line">sysctl -w net.ipv4.ipfrag_time=60</span><br></pre></td></tr></table></figure>

<h4 id="frag-high-thresh-分片的内存超过一定阈值会导致系统安全检查丢包"><a href="#frag-high-thresh-分片的内存超过一定阈值会导致系统安全检查丢包" class="headerlink" title="frag_high_thresh, 分片的内存超过一定阈值会导致系统安全检查丢包"></a>frag_high_thresh, 分片的内存超过一定阈值会导致系统安全检查丢包</h4><p>查看：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">netstat -s|grep reassembles</span><br><span class="line">8094 packet reassembles failed</span><br></pre></td></tr></table></figure>

<p>解决方案：调整大小</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">net.ipv4.ipfrag_high_thresh </span><br><span class="line">net.ipv4.ipfrag_low_thresh</span><br></pre></td></tr></table></figure>

<h4 id="分片安全距检查离丢包"><a href="#分片安全距检查离丢包" class="headerlink" title="分片安全距检查离丢包"></a>分片安全距检查离丢包</h4><p>查看：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">netstat -s|grep reassembles</span><br><span class="line">8094 packet reassembles failed</span><br></pre></td></tr></table></figure>

<p>解决方案： 把 ipfrag_max_dist 设置为0，就关掉此安全检查</p>
<p><img src="/img/640-17.png" alt="640-17.png"></p>
<p>pfrag_max_dist 特性，在一些场景下其实并不适用：</p>
<ol>
<li>有大量的网络报文交互</li>
<li>发送端的并发度很高，同时SMP架构，导致很容易造成这种乱序情况；</li>
</ol>
<p>分片 hash bucket 冲突链太长超过系统默认值128</p>
<p>查看：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">dmesg|grep <span class="string">&quot;Dropping fragment&quot;</span></span><br><span class="line">inet_frag_find: Fragment <span class="built_in">hash</span> bucket 128 list length grew over <span class="built_in">limit</span>. Dropping fragment.</span><br></pre></td></tr></table></figure>

<p>解决方案：热补丁调整hash大小；</p>
<h4 id="系统内存不足，创建新分片队列失败"><a href="#系统内存不足，创建新分片队列失败" class="headerlink" title="系统内存不足，创建新分片队列失败"></a>系统内存不足，创建新分片队列失败</h4><p>查看方法：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">netstat -s|grep reassembles</span><br><span class="line">8094 packet reassembles failed</span><br></pre></td></tr></table></figure>

<p>dropwatch查看丢包位置 ：</p>
<p><img src="/img/640-18.png" alt="640-18.png"></p>
<p>解决方案：</p>
<ol>
<li>增大系统网络内存：</li>
</ol>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">net.core.rmem_default </span><br><span class="line">net.core.rmem_max </span><br><span class="line">net.core.wmem_default</span><br></pre></td></tr></table></figure>

<ol start="2">
<li>系统回收内存：</li>
</ol>
<p>紧急情况下，可以用 <code>/proc/sys/vm/drop_caches</code>, 去释放一下虚拟内存；</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># To free pagecache:</span></span><br><span class="line"><span class="built_in">echo</span> 1 &gt; /proc/sys/vm/drop_caches</span><br><span class="line"></span><br><span class="line"><span class="comment"># To free dentries and inodes:</span></span><br><span class="line"><span class="built_in">echo</span> 2 &gt; /proc/sys/vm/drop_caches</span><br><span class="line"></span><br><span class="line"><span class="comment"># To free pagecache, dentries and inodes:</span></span><br><span class="line"><span class="built_in">echo</span> 3 &gt; /proc/sys/vm/drop_caches</span><br></pre></td></tr></table></figure>

<h4 id="MTU丢包"><a href="#MTU丢包" class="headerlink" title="MTU丢包"></a>MTU丢包</h4><p><img src="/img/640-19.png" alt="640-19.png"></p>
<p>查看：</p>
<ol>
<li>检查接口MTU配置，<code>ifconfig eth1/eth0</code>，默认是1500；</li>
<li>进行MTU探测，然后设置接口对应的MTU值；</li>
</ol>
<p>解决方案：</p>
<ol>
<li>根据实际情况，设置正确 MTU 值；</li>
<li>设置合理的 tcp mss，启用 TCP MTU Probe:</li>
</ol>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cat</span> /proc/sys/net/ipv4/tcp_mtu_probing</span><br><span class="line">tcp_mtu_probing - INTEGER Controls TCP Packetization-Layer Path MTU Discovery.</span><br><span class="line">Takes three values:</span><br><span class="line">0 - Disabled </span><br><span class="line">1 - Disabled by default, enabled when an ICMP black hole detected</span><br><span class="line">2 - Always enabled, use initial MSS of tcp_base_mss.</span><br></pre></td></tr></table></figure>

<h4 id="tcp层丢包"><a href="#tcp层丢包" class="headerlink" title="tcp层丢包"></a>tcp层丢包</h4><h5 id="TIME-WAIT-过多丢包"><a href="#TIME-WAIT-过多丢包" class="headerlink" title="TIME_WAIT 过多丢包"></a>TIME_WAIT 过多丢包</h5><p>大量 TIMEWAIT 出现，并且需要解决的场景，在高并发短连接的TCP服务器上，当服务器处理完请求后立刻按照主动正常关闭连接。。。这个场景下，会出现大量socket处于TIMEWAIT状态。如果客户端的并发量持续很高，此时部分客户端就会显示连接不上；</p>
<p>查看：</p>
<p>查看系统log ：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">dmsg</span><br><span class="line">TCP: time <span class="built_in">wait</span> bucket table overflow；</span><br></pre></td></tr></table></figure>

<p>查看系统配置：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sysctl -a|grep tcp_max_tw_buckets</span><br><span class="line">net.ipv4.tcp_max_tw_buckets = 16384</span><br></pre></td></tr></table></figure>

<p>解决方案：</p>
<ol>
<li>tw_reuse，tw_recycle 必须在客户端和服务端 timestamps 开启时才管用（默认打开）</li>
<li>tw_reuse 只对客户端起作用，开启后客户端在1s内回收；</li>
<li>tw_recycle 对客户端和服务器同时起作用，开启后在 3.5*RTO 内回收，RTO 200ms~ 120s具体时间视网络状况。内网状况比 tw_reuse 稍快，公网尤其移动网络大多要比 tw_reuse 慢，优点就是能够回收服务端的 TIME_WAIT 数量；在服务端，如果网络路径会经过NAT节点，不要启用 net.ipv4.tcp_tw_recycle，会导致时间戳混乱，引起其他丢包问题；</li>
<li>调整 tcp_max_tw_buckets 大小，如果内存足够：</li>
</ol>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sysctl -w net.ipv4.tcp_max_tw_buckets=163840；</span><br></pre></td></tr></table></figure>

<h5 id="时间戳异常丢包"><a href="#时间戳异常丢包" class="headerlink" title="时间戳异常丢包"></a>时间戳异常丢包</h5><p>当多个客户端处于同一个 NAT 环境时，同时访问服务器，不同客户端的时间可能不一致，此时服务端接收到同一个NAT发送的请求，就会出现时间戳错乱的现象，于是后面的数据包就被丢弃了，具体的表现通常是是客户端明明发送的SYN，但服务端就是不响应ACK。在服务器借助下面的命令可以来确认数据包是否有不断被丢弃的现象。</p>
<p>检查：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">netstat -s | grep rejects</span><br></pre></td></tr></table></figure>

<p>解决方案：</p>
<p>如果网络路径会经过NAT节点，不要启用 <code>net.ipv4.tcp_tw_recycle</code>；</p>
<h5 id="TCP队列问题导致丢包"><a href="#TCP队列问题导致丢包" class="headerlink" title="TCP队列问题导致丢包"></a>TCP队列问题导致丢包</h5><p>原理：</p>
<p>tcp状态机（三次握手）</p>
<p><img src="/img/640-20.png" alt="640-20.png"></p>
<p>协议处理：</p>
<p><img src="/img/640-8.webp" alt="640-8.webp"></p>
<p><strong>一个是半连接队列（syn queue）：</strong></p>
<p>在三次握手协议中，服务器维护一个半连接队列，该队列为每个客户端的SYN包开设一个条目(服务端在接收到SYN包的时候，就已经创建了 request_sock 结构，存储在半连接队列中)，该条目表明服务器已收到SYN包，并向客户发出确认，正在等待客户的确认包（会进行第二次握手发送SYN＋ACK的包加以确认）。这些条目所标识的连接在服务器处于Syn_RECV 状态，当服务器收到客户的确认包时，删除该条目，服务器进入 ESTABLISHED 状态。该队列为SYN队列，长度为 max(64,/proc/sys/net/ipv4/tcp_max_syn_backlog),  机器的 tcp_max_syn_backlog 值在 /proc/sys/net/ipv4/tcp_max_syn_backlog 下配置;</p>
<p><strong>一个是全连接队列（accept queue）：</strong></p>
<p>第三次握手时，当server接收到ACK 报之后， 会进入一个新的叫 accept 的队列，该队列的长度为 min(backlog, somaxconn)，默认情况下，somaxconn 的值为 128，表示最多有 129 的 ESTAB 的连接等待 accept()，而 backlog 的值则应该是由 int listen(int sockfd, int backlog) 中的第二个参数指定，listen 里面的 backlog 可以有我们的应用程序去定义的;</p>
<p>查看：</p>
<p>连接建立失败,syn丢包：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">netstat -s |grep -i listen</span><br><span class="line">SYNs to LISTEN sockets dropped</span><br></pre></td></tr></table></figure>

<p>也会受到连接满丢包影响</p>
<p>解决方案： 增加大小 <code>tcp_max_syn_backlog</code></p>
<h5 id="连接满丢包"><a href="#连接满丢包" class="headerlink" title="连接满丢包"></a>连接满丢包</h5><p><code>-xxx times the listen queue of a socket overflowed</code></p>
<p>查看：</p>
<ul>
<li>查看 accept队列大小 ：<code>net.core.somaxconn</code></li>
<li>ss -lnt 查询socket 队列 ：LISTEN 状态: Recv-Q 表示的当前等待服务端调用 accept 完成三次握手的 listen backlog 数值，也就是说，当客户端通过 connect() 去连接正在 listen() 的服务端时，这些连接会一直处于这个 queue 里面直到被服务端 accept()；Send-Q 表示的则是最大的 listen backlog 数值，这就就是上面提到的 min(backlog, somaxconn) 的值，</li>
<li>看一下是不是应用程序设置限制， int listen(int sockfd, int backlog)；</li>
</ul>
<p>解决方案：</p>
<ul>
<li>Linux内核参进行优化，可以缓解压力 tcp_abort_on_overflow=1</li>
<li>调整 net.core.somaxconn 大小;</li>
<li>应用程序设置问题，通知客户程序修改；</li>
</ul>
<h5 id="syn-flood攻击丢包"><a href="#syn-flood攻击丢包" class="headerlink" title="syn flood攻击丢包"></a>syn flood攻击丢包</h5><p>目前，Linux 下默认会进行5次重发SYN-ACK包，重试的间隔时间从1s开始，下次的重试间隔时间是前一次的双倍，5次的重试时间间隔为1s, 2s, 4s, 8s, 16s，总共31s，第5次发出后还要等32s都知道第5次也超时了，所以，总共需要 1s + 2s + 4s+ 8s+ 16s + 32s = 63s，TCP才会把断开这个连接。由于，SYN超时需要63秒，那么就给攻击者一个攻击服务器的机会，攻击者在短时间内发送大量的SYN包给Server(俗称 SYN flood 攻击)，用于耗尽Server的SYN队列。对于应对SYN 过多的问题;</p>
<p>查看：  </p>
<p>查看syslog：  </p>
<p>kernel: [3649830.269068] TCP: Possible SYN flooding on port xxx. Sending cookies. Check SNMP counters.</p>
<p>解决方案：</p>
<ul>
<li>增大tcp_max_syn_backlog</li>
<li>减少tcp_synack_retries</li>
<li>启用tcp_syncookies</li>
<li>启用 tcp_abort_on_overflow， tcp_abort_on_overflow 修改成 1，1表示第三步的时候如果全连接队列满了，server发送一个reset包给client，表示废掉这个握手过程和这个连接（本来在server端这个连接就还没建立起来）；</li>
</ul>
<h5 id="PAWS机制丢包"><a href="#PAWS机制丢包" class="headerlink" title="PAWS机制丢包"></a>PAWS机制丢包</h5><p>原理：PAWS(Protect Against Wrapped Sequence numbers)，高带宽下，TCP序列号可能在较短的时间内就被重复使用(recycle/wrapped)，就可能导致同一条TCP流在短时间内出现序号一样的两个合法的数据包及其确认包。</p>
<p>查看：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">netstat -s |grep -e <span class="string">&quot;passive connections rejected because of time </span></span><br><span class="line"><span class="string">stamp&quot;</span> -e <span class="string">&quot;packets rejects in established connections because of </span></span><br><span class="line"><span class="string">timestamp&quot;</span> </span><br><span class="line">387158 passive connections rejected because of time stamp</span><br><span class="line">825313 packets rejects <span class="keyword">in</span> established connections because of timestamp</span><br></pre></td></tr></table></figure>

<p>通过 sysctl 查看是否启用了 tcp_tw_recycle 及 tcp_timestamp:</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ sysctl net.ipv4.tcp_tw_recycle</span><br><span class="line">net.ipv4.tcp_tw_recycle = 1 </span><br><span class="line"></span><br><span class="line">$ sysctl net.ipv4.tcp_timestamps</span><br><span class="line">net.ipv4.tcp_timestamps = 1</span><br></pre></td></tr></table></figure>

<ol>
<li>tcp_tw_recycle 参数。它用来快速回收 TIME_WAIT 连接，不过如果在NAT环境下会引发问题;</li>
<li>当多个客户端通过NAT方式联网并与服务端交互时，服务端看到的是同一个IP，也就是说对服务端而言这些客户端实际上等同于一个，可惜由于这些客户端的时间戳可能存在差异，于是乎从服务端的视角看，便可能出现时间戳错乱的现象，进而直接导致时间戳小的数据包被丢弃。如果发生了此类问题，具体的表现通常是是客户端明明发送的SYN，但服务端就是不响应ACK。</li>
</ol>
<p>解决方案： </p>
<p>在NAT环境下，清除tcp时间戳选项，或者不开启 tcp_tw_recycle 参数；</p>
<h5 id="TLP问题丢包"><a href="#TLP问题丢包" class="headerlink" title="TLP问题丢包"></a>TLP问题丢包</h5><p>TLP主要是为了解决尾丢包重传效率的问题，TLP能够有效的避免较长的RTO超时，进而提高TCP性能，详细参考文章：</p>
<p><a target="_blank" rel="noopener" href="http://perthcharles.github.io/2015/10/31/wiki-network-tcp-tlp/">http://perthcharles.github.io/2015/10/31/wiki-network-tcp-tlp/</a>；</p>
<p>但在低时延场景下（短连接小包量），TLP与延迟ACK组合可能会造成无效重传，导致客户端感发现大量假重传包，加大了响应延迟；</p>
<p>查看：</p>
<p>查看协议栈统计：</p>
<p><code>netstat -s |grep TCPLossProbes</code></p>
<p>查看系统配置：</p>
<p><code>sysctl -a | grep tcp_early_retrans</code></p>
<p><img src="/img/640-21.png" alt="640-21.png"></p>
<p>解决方案：</p>
<ol>
<li>关掉延迟ack，打开快速ack；</li>
<li>linux实现nodelay语意不是快速ack，只是关闭nagle算法；</li>
<li>打开快速ack选项，socket里面有个 TCP_QUICKACK 选项， 需要每次recv后再设置一次。</li>
</ol>
<h5 id="内存不足导致丢包"><a href="#内存不足导致丢包" class="headerlink" title="内存不足导致丢包"></a>内存不足导致丢包</h5><p>查看：</p>
<p>查看log：<br><code>dmesg|grep &quot;out of memory&quot;</code></p>
<p>查看系统配置： </p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cat</span> /proc/sys/net/ipv4/tcp_mem</span><br><span class="line"><span class="built_in">cat</span> /proc/sys/net/ipv4/tcp_rmem</span><br><span class="line"><span class="built_in">cat</span> /proc/sys/net/ipv4/tcp_wmem</span><br></pre></td></tr></table></figure>

<p>解决方案：</p>
<p>根据TCP业务并发流量，调整系统参数，一般试着增大2倍或者其他倍数来看是否缓解；</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sysclt -w net.ipv4.tcp_mem=</span><br><span class="line">sysclt -w net.ipv4.tcp_wmem=</span><br><span class="line">sysclt -w net.ipv4.tcp_rmem=</span><br><span class="line">sysctl -p</span><br></pre></td></tr></table></figure>

<h5 id="TCP超时丢包"><a href="#TCP超时丢包" class="headerlink" title="TCP超时丢包"></a>TCP超时丢包</h5><p>查看：</p>
<p>抓包分析一下网络RTT：</p>
<p><img src="/img/640-9.webp" alt="640-9.webp"></p>
<p>用其他工具测试一下当前端到端网络质量（hping等）；</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">hping -S 9.199.10.104 -A</span><br><span class="line">HPING 9.199.10.104 (bond1 9.199.10.104): SA <span class="built_in">set</span>, 40 headers + 0 data bytes</span><br><span class="line">len=46 ip=9.199.10.104 ttl=53 DF <span class="built_in">id</span>=47617 sport=0 flags=R <span class="built_in">seq</span>=0 win=0 rtt=38.3 ms</span><br><span class="line">len=46 ip=9.199.10.104 ttl=53 DF <span class="built_in">id</span>=47658 sport=0 flags=R <span class="built_in">seq</span>=1 win=0 rtt=38.3 ms</span><br><span class="line">len=46 ip=9.199.10.104 ttl=53 DF <span class="built_in">id</span>=47739 sport=0 flags=R <span class="built_in">seq</span>=2 win=0 rtt=30.4 ms</span><br><span class="line">len=46 ip=9.199.10.104 ttl=53 DF <span class="built_in">id</span>=47842 sport=0 flags=R <span class="built_in">seq</span>=3 win=0 rtt=30.4 ms</span><br><span class="line">len=46 ip=9.199.10.104 ttl=53 DF <span class="built_in">id</span>=48485 sport=0 flags=R <span class="built_in">seq</span>=4 win=0 rtt=38.7 ms</span><br><span class="line">len=46 ip=9.199.10.104 ttl=53 DF <span class="built_in">id</span>=49274 sport=0 flags=R <span class="built_in">seq</span>=5 win=0 rtt=34.1 ms</span><br><span class="line">len=46 ip=9.199.10.104 ttl=53 DF <span class="built_in">id</span>=49491 sport=0 flags=R <span class="built_in">seq</span>=6 win=0 rtt=30.3 ms</span><br></pre></td></tr></table></figure>

<p>解决方案：</p>
<ul>
<li><p>关闭Nagle算法，减少小包延迟；</p>
</li>
<li><p>关闭延迟ack: <code>sysctl -w net.ipv4.tcp_no_delay_ack=1</code></p>
</li>
</ul>
<h5 id="TCP乱序丢包"><a href="#TCP乱序丢包" class="headerlink" title="TCP乱序丢包"></a>TCP乱序丢包</h5><p>此时TCP会无法判断是数据包丢失还是乱序，因为丢包和乱序都会导致接收端收到次序混乱的数据包，造成接收端的数据空洞。TCP会将这种情况暂定为数据包的乱序，因为乱序是时间问题（可能是数据包的迟到），而丢包则意味着重传。当TCP意识到包出现乱序的情况时，会立即ACK，该ACK的TSER部分包含的TSEV值会记录当前接收端收到有序报文段的时刻。这会使得数据包的RTT样本值增大，进一步导致RTO时间延长。这对TCP来说无疑是有益的，因为TCP有充分的时间判断数据包到底是失序还是丢了来防止不必要的数据重传。当然严重的乱序则会让发送端以为是丢包一旦重复的ACK超过TCP的阈值，便会触发超时重传机制，以及时解决这种问题；详细请参考博客：</p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/dog250/article/details/78692585">https://blog.csdn.net/dog250/article/details/78692585</a></p>
<p>查看：抓包分析是否存在很多乱序报文：</p>
<p><img src="/img/640-10.webp" alt="640-10.webp"></p>
<p>解决方案：如果在多径传输场景或者网络质量不好，可以通过修改下面值来提供系统对TCP无序传送的容错率：</p>
<p><img src="/img/640-11.webp" alt="640-11.webp"></p>
<h5 id="拥塞控制丢包"><a href="#拥塞控制丢包" class="headerlink" title="拥塞控制丢包"></a>拥塞控制丢包</h5><p>在互联网发展的过程当中，TCP算法也做出了一定改变，先后演进了</p>
<p>Reno、NewReno、Cubic和Vegas，这些改进算法大体可以分为基于丢包和基于延时的拥塞控制算法。基于丢包的拥塞控制算法以Reno、NewReno为代表，它的主要问题有Buffer bloat和长肥管道两种，基于丢包的协议拥塞控制机制是被动式的，其依据网络中的丢包事件来做网络拥塞判断。即使网络中的负载很高，只要没有产生拥塞丢包，协议就不会主动降低自己的发送速度。最初路由器转发出口的Buffer 是比较小的，TCP在利用时容易造成全局同步，降低带宽利用率，随后路由器厂家由于硬件成本下降不断地增加Buffer，基于丢包反馈的协议在不丢包的情况下持续占用路由器buffer，虽然提高了网络带宽的利用率，但同时也意味着发生拥塞丢包后，网络抖动性加大。另外对于带宽和RTT都很高的长肥管道问题来说，管道中随机丢包的可能性很大，TCP的默认buffer设置比较小加上随机丢包造成的cwnd经常下折，导致带宽利用率依旧很低；  BBR（Bottleneck Bandwidth and Round-trip propagation time）是一种基于带宽和延迟反馈的拥塞控制算法。目前已经演化到第二版，是一个典型的封闭反馈系统，发送多少报文和用多快的速度发送这些报文都是在每次反馈中不断调节。在BBR提出之前，拥塞控制都是基于事件的算法，需要通过丢包或延时事件驱动；BBR提出之后，拥塞控制是基于反馈的自主自动控制算法，对于速率的控制是由算法决定，而不由网络事件决定，BBR算法的核心是找到最大带宽（Max BW）和最小延时（Min RTT）这两个参数，最大带宽和最小延时的乘积可以得到BDP(Bandwidth Delay Product), 而BDP就是网络链路中可以存放数据的最大容量。BDP驱动Probing State Machine得到Rate quantum和cwnd，分别设置到发送引擎中就可以解决发送速度和数据量的问题。</p>
<p>Linux 4.9内核首次采用BBR拥塞控制算法第一个版本，BBR抗丢包能力比其他算法要强，但这个版本在某些场景下面有问题（缺点），BBR在实时音视频领域存在的问题，深队列竞争不过Cubic。</p>
<p>问题现象就是：在深队列场景，BBR的ProbeRTT阶段只发4个包，发送速率下降太多会引发延迟加大和卡顿问题。</p>
<p>查看：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ss -sti //在源端 ss -sti | grep 10.125.42.49:47699 -A 3 <span class="comment">#（10.125.42.49:47699 是目的端地址和端口号）</span></span><br></pre></td></tr></table></figure>

<p><img src="/img/640-12.webp" alt="640-12.webp"></p>
<p><img src="/img/640-13.webp" alt="640-13.webp"></p>
<p>解决方案：</p>
<ul>
<li>ProbeRTT并不适用实时音视频领域，因此可以选择直接去除，或者像BBRV2把probe RTT缩短到2.5s一次，使用0.5xBDP发送；</li>
<li>如果没有特殊需求，切换成稳定的cubic算法；</li>
</ul>
<h4 id="UDP层丢包"><a href="#UDP层丢包" class="headerlink" title="UDP层丢包"></a>UDP层丢包</h4><p>收发包失败丢包</p>
<p>查看：netstat 统计</p>
<p>如果有持续的 receive buffer errors/send buffer errors 计数；</p>
<p><img src="/img/640-22.png" alt="640-22.png"></p>
<p>解决方案：</p>
<ol>
<li>CPU负载（多核绑核配置），网络负载（软中断优化，调整驱动队列netdev_max_backlog），内存配置（协议栈内存）；</li>
<li>按峰值在来，增大buffer缓存区大小：</li>
</ol>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">net.ipv4.udp_mem = xxx</span><br><span class="line">net.ipv4.udp_rmem_min = xxx</span><br><span class="line">net.ipv4.udp_wmem_min = xxx</span><br></pre></td></tr></table></figure>

<ol start="3">
<li>调整应用设计：</li>
</ol>
<ul>
<li>UDP本身就是无连接不可靠的协议，适用于报文偶尔丢失也不影响程序状态的场景，比如视频、音频、游戏、监控等。对报文可靠性要求比较高的应用不要使用 UDP，推荐直接使用 TCP。当然，也可以在应用层做重试、去重保证可靠性</li>
<li>如果发现服务器丢包，首先通过监控查看系统负载是否过高，先想办法把负载降低再看丢包问题是否消失</li>
<li>如果系统负载过高，UDP丢包是没有有效解决方案的。如果是应用异常导致CPU、memory、IO 过高，请及时定位异常应用并修复；如果是资源不够，监控应该能及时发现并快速扩容</li>
<li>对于系统大量接收或者发送UDP报文的，可以通过调节系统和程序的 socket buffer size 来降低丢包的概率</li>
<li>应用程序在处理UDP报文时，要采用异步方式，在两次接收报文之间不要有太多的处理逻辑</li>
</ul>
<h3 id="应用层socket丢包"><a href="#应用层socket丢包" class="headerlink" title="应用层socket丢包"></a>应用层socket丢包</h3><h4 id="socket缓存区接收丢包"><a href="#socket缓存区接收丢包" class="headerlink" title="socket缓存区接收丢包"></a>socket缓存区接收丢包</h4><p>查看：  </p>
<ol>
<li>抓包分析是否存在丢包情况；</li>
<li>查看统计：</li>
</ol>
<p><code>netstat -s|grep &quot;packet receive errors&quot;</code></p>
<p>解决方案：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 调整socket缓冲区大小：</span></span><br><span class="line"><span class="comment"># socket配置（所有协议socket）：</span></span><br><span class="line"><span class="comment"># Default Socket Receive Buffer</span></span><br><span class="line">net.core.rmem_default = 31457280</span><br><span class="line"></span><br><span class="line"><span class="comment"># Maximum Socket Receive Buffer</span></span><br><span class="line">net.core.rmem_max = 67108864</span><br></pre></td></tr></table></figure>

<p>具体大小调整原理：</p>
<p>缓冲区大小没有任何设置值是最佳的，因为最佳大小随具体情况而不同</p>
<p>缓冲区估算原理：在数据通信中，带宽时延乘积（英语：bandwidth-delay product；或称带宽延时乘积、带宽延时积等）指的是一个数据链路的能力（每秒比特）与来回通信延迟（单位秒）的乘积。[1][2]其结果是以比特（或字节）为单位的一个数据总量，等同在任何特定时间该网络线路上的最大数据量——已发送但尚未确认的数据。</p>
<p><strong>BDP = 带宽 * RTT</strong></p>
<p>可以通过计算当面节点带宽和统计平均时延来估算BDP，即缓冲区的大小，可以参考下面常见场景估计：</p>
<p><img src="/img/640-23.png" alt="640-23.png"></p>
<p>参考：<a target="_blank" rel="noopener" href="https://docs.oracle.com/cd/E56344_01/html/E53803/gnkor.html">https://docs.oracle.com/cd/E56344_01/html/E53803/gnkor.html</a></p>
<h4 id="应用设置tcp连接数大小丢包"><a href="#应用设置tcp连接数大小丢包" class="headerlink" title="应用设置tcp连接数大小丢包"></a>应用设置tcp连接数大小丢包</h4><p>查看：</p>
<p>请参考上面TCP连接队列分析；</p>
<p>解决方案：</p>
<p>设置合理的连接队列大小，当第三次握手时，当server接收到ACK 报之后， 会进入一个新的叫 accept 的队列，该队列的长度为 min(backlog, somaxconn)，默认情况下，somaxconn 的值为 128，表示最多有 129 的 ESTAB 的连接等待 accept()，而 backlog 的值则应该是由 int listen(int sockfd, int backlog) 中的第二个参数指定，listen 里面的 backlog 可以有我们的应用程序去定义的；</p>
<h4 id="应用发送太快导致丢包"><a href="#应用发送太快导致丢包" class="headerlink" title="应用发送太快导致丢包"></a>应用发送太快导致丢包</h4><p>查看统计：</p>
<p><code>netstat -s|grep &quot;send buffer errors&quot;</code></p>
<p>解决方案：</p>
<ul>
<li>ICMP/UDP没有流控机制，需要应用设计合理发送方式和速度，照顾到底层buff大小和CPU负载以及网络带宽质量；</li>
<li>设置合理的sock缓冲区大小：<br><code>setsockopt(s,SOL_SOCKET,SO_SNDBUF,  i(const char*)&amp;nSendBuf,sizeof(int));</code></li>
<li>调整系统socket缓冲区大小：</li>
</ul>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Default Socket Send Buffer</span></span><br><span class="line">net.core.wmem_default = 31457280</span><br><span class="line"><span class="comment"># Maximum Socket Send Buffer</span></span><br><span class="line">net.core.wmem_max = 33554432</span><br></pre></td></tr></table></figure>

<p>附：简单总结一下内核协议栈丢包：</p>
<p><img src="/img/640-24.png" alt="640-24.png"></p>
<h2 id="相关工具介绍"><a href="#相关工具介绍" class="headerlink" title="相关工具介绍"></a>相关工具介绍</h2><p>1.dropwatch工具</p>
<p>原理：  监听 kfree_skb（把网络报文丢弃时会调用该函数）函数或者事件吗，然后打印对应调用堆栈；想要详细了解 linux 系统在执行哪个函数时丢包的话，可以使用 dropwatch 工具，它监听系统丢包信息，并打印出丢包发生的函数：</p>
<p><img src="/img/640-25.png" alt="640-25.png"></p>
<ol start="2">
<li>tcpdump工具</li>
</ol>
<p>原理: tcpdump 是一个Unix下一个功能强大的网络抓包工具，它允许用户拦截和显示发送或收到过网络连接到该计算机的TCP/IP和其他数据包</p>
<p><img src="/img/640-26.png" alt="640-26.png"></p>
<p>抓包命令参考：</p>
<p><a target="_blank" rel="noopener" href="https://www.tcpdump.org/manpages/tcpdump.1.html">https://www.tcpdump.org/manpages/tcpdump.1.html</a></p>
<p>数据包分析：</p>
<p>1.用wireshark工具分析  参考：Wireshark数据包分析实战.pdf<br>2.可以转化生成CSV数据，用Excel或者shell去分析特定场景报文；<br>3.可以在linux上用tshark命令行工具进行分析:</p>
<p><a target="_blank" rel="noopener" href="https://www.wireshark.org/docs/man-pages/tshark.html">https://www.wireshark.org/docs/man-pages/tshark.html</a></p>

    </div>

    
    
    

      <footer class="post-footer">
          
          <div class="post-tags">
              <a href="/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/" rel="tag"><i class="fa fa-tag"></i> 计算机基础</a>
              <a href="/tags/%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80/" rel="tag"><i class="fa fa-tag"></i> 网络基础</a>
              <a href="/tags/TCP%E5%8D%8F%E8%AE%AE/" rel="tag"><i class="fa fa-tag"></i> TCP协议</a>
              <a href="/tags/Linux%E7%BD%91%E7%BB%9C/" rel="tag"><i class="fa fa-tag"></i> Linux网络</a>
              <a href="/tags/IP%E7%BD%91%E7%BB%9C/" rel="tag"><i class="fa fa-tag"></i> IP网络</a>
              <a href="/tags/Ethernet/" rel="tag"><i class="fa fa-tag"></i> Ethernet</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2021/03/01/%E7%BD%91%E7%BB%9C%E6%95%B0%E6%8D%AE%E5%8C%85%E5%8F%91%E9%80%81%E5%92%8C%E6%8E%A5%E6%94%B6%E8%BF%87%E7%A8%8B%E8%A7%A3%E6%9E%90/" rel="prev" title="网络数据包发送和接收过程解析">
      <i class="fa fa-chevron-left"></i> 网络数据包发送和接收过程解析
    </a></div>
      <div class="post-nav-item">
    <a href="/2021/03/08/%E5%BF%AB%E9%80%9F%E8%AE%A1%E7%AE%97%E5%B9%BF%E6%92%AD%E5%9C%B0%E5%9D%80/" rel="next" title="快速计算广播地址">
      快速计算广播地址 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%A1%AC%E4%BB%B6%E7%BD%91%E5%8D%A1%E4%B8%A2%E5%8C%85"><span class="nav-number">1.</span> <span class="nav-text">硬件网卡丢包</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Ring-Buffer%E6%BA%A2%E5%87%BA"><span class="nav-number">1.1.</span> <span class="nav-text">Ring Buffer溢出</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%BD%91%E5%8D%A1%E7%AB%AF%E5%8F%A3%E5%8D%8F%E5%95%86%E4%B8%A2%E5%8C%85"><span class="nav-number">1.2.</span> <span class="nav-text">网卡端口协商丢包</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%BD%91%E5%8D%A1%E6%B5%81%E6%8E%A7%E4%B8%A2%E5%8C%85"><span class="nav-number">1.3.</span> <span class="nav-text">网卡流控丢包</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%8A%A5%E6%96%87mac%E5%9C%B0%E5%9D%80%E4%B8%A2%E5%8C%85"><span class="nav-number">1.4.</span> <span class="nav-text">报文mac地址丢包</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%85%B6%E4%BB%96%E7%BD%91%E5%8D%A1%E5%BC%82%E5%B8%B8%E4%B8%A2%E5%8C%85"><span class="nav-number">1.5.</span> <span class="nav-text">其他网卡异常丢包</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%BD%91%E5%8D%A1firmware%E7%89%88%E6%9C%AC"><span class="nav-number">1.5.1.</span> <span class="nav-text">网卡firmware版本:</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%BD%91%E7%BA%BF%E6%8E%A5%E8%A7%A6%E4%B8%8D%E8%89%AF%EF%BC%9A"><span class="nav-number">1.5.2.</span> <span class="nav-text">网线接触不良：</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%8A%A5%E6%96%87%E9%95%BF%E5%BA%A6%E4%B8%A2%E5%8C%85"><span class="nav-number">1.5.3.</span> <span class="nav-text">报文长度丢包</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%BD%91%E5%8D%A1%E9%A9%B1%E5%8A%A8%E4%B8%A2%E5%8C%85"><span class="nav-number">2.</span> <span class="nav-text">网卡驱动丢包</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%A9%B1%E5%8A%A8%E6%BA%A2%E5%87%BA%E4%B8%A2%E5%8C%85"><span class="nav-number">2.1.</span> <span class="nav-text">驱动溢出丢包</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8D%95%E6%A0%B8%E8%B4%9F%E8%BD%BD%E9%AB%98%E5%AF%BC%E8%87%B4%E4%B8%A2%E5%8C%85"><span class="nav-number">2.2.</span> <span class="nav-text">单核负载高导致丢包</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%86%85%E6%A0%B8%E5%8D%8F%E8%AE%AE%E6%A0%88%E4%B8%A2%E5%8C%85"><span class="nav-number">3.</span> <span class="nav-text">内核协议栈丢包</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BB%A5%E5%A4%AA%E7%BD%91%E9%93%BE%E8%B7%AF%E5%B1%82%E4%B8%A2%E5%8C%85"><span class="nav-number">3.1.</span> <span class="nav-text">以太网链路层丢包</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#neighbor-%E7%B3%BB%E7%BB%9Farp%E4%B8%A2%E5%8C%85"><span class="nav-number">3.1.1.</span> <span class="nav-text">neighbor 系统arp丢包</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#arp-ignore-%E9%85%8D%E7%BD%AE%E4%B8%A2%E5%8C%85"><span class="nav-number">3.1.1.1.</span> <span class="nav-text">arp_ignore 配置丢包</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#arp-filter%E9%85%8D%E7%BD%AE%E4%B8%A2%E5%8C%85"><span class="nav-number">3.1.1.2.</span> <span class="nav-text">arp_filter配置丢包</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#arp%E8%A1%A8%E6%BB%A1%E5%AF%BC%E8%87%B4%E4%B8%A2%E5%8C%85"><span class="nav-number">3.1.2.</span> <span class="nav-text">arp表满导致丢包</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#arp%E8%AF%B7%E6%B1%82%E7%BC%93%E5%AD%98%E9%98%9F%E5%88%97%E6%BA%A2%E5%87%BA%E4%B8%A2%E5%8C%85"><span class="nav-number">3.1.3.</span> <span class="nav-text">arp请求缓存队列溢出丢包</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%BD%91%E7%BB%9CIP%E5%B1%82%E4%B8%A2%E5%8C%85"><span class="nav-number">3.2.</span> <span class="nav-text">网络IP层丢包</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%8E%A5%E5%8F%A3ip%E5%9C%B0%E5%9D%80%E9%85%8D%E7%BD%AE%E4%B8%A2%E5%8C%85"><span class="nav-number">3.2.1.</span> <span class="nav-text">接口ip地址配置丢包</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%B7%AF%E7%94%B1%E4%B8%A2%E5%8C%85"><span class="nav-number">3.2.2.</span> <span class="nav-text">路由丢包</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E8%B7%AF%E7%94%B1%E9%85%8D%E7%BD%AE%E4%B8%A2%E5%8C%85"><span class="nav-number">3.2.2.1.</span> <span class="nav-text">路由配置丢包</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%8F%8D%E5%90%91%E8%B7%AF%E7%94%B1%E8%BF%87%E6%BB%A4%E4%B8%A2%E5%8C%85"><span class="nav-number">3.2.2.2.</span> <span class="nav-text">反向路由过滤丢包</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%98%B2%E7%81%AB%E5%A2%99%E4%B8%A2%E5%8C%85"><span class="nav-number">3.2.3.</span> <span class="nav-text">防火墙丢包</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%AE%A2%E6%88%B7%E8%AE%BE%E7%BD%AE%E8%A7%84%E5%88%99%E5%AF%BC%E8%87%B4%E4%B8%A2%E5%8C%85"><span class="nav-number">3.2.3.1.</span> <span class="nav-text">客户设置规则导致丢包</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%BF%9E%E6%8E%A5%E8%B7%9F%E8%B8%AA%E5%AF%BC%E8%87%B4%E4%B8%A2%E5%8C%85"><span class="nav-number">3.2.4.</span> <span class="nav-text">连接跟踪导致丢包</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#ct%E5%88%9B%E5%BB%BA%E5%86%B2%E7%AA%81%E5%A4%B1%E5%AF%BC%E8%87%B4%E4%B8%A2%E5%8C%85"><span class="nav-number">3.2.5.</span> <span class="nav-text">ct创建冲突失导致丢包</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BC%A0%E8%BE%93%E5%B1%82UDP-TCP%E4%B8%A2%E5%8C%85"><span class="nav-number">3.3.</span> <span class="nav-text">传输层UDP&#x2F;TCP丢包</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#tcp-%E8%BF%9E%E6%8E%A5%E8%B7%9F%E8%B8%AA%E5%AE%89%E5%85%A8%E6%A3%80%E6%9F%A5%E4%B8%A2%E5%8C%85"><span class="nav-number">3.3.1.</span> <span class="nav-text">tcp 连接跟踪安全检查丢包</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%88%86%E7%89%87%E9%87%8D%E7%BB%84%E4%B8%A2%E5%8C%85"><span class="nav-number">3.3.2.</span> <span class="nav-text">分片重组丢包</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#frag-high-thresh-%E5%88%86%E7%89%87%E7%9A%84%E5%86%85%E5%AD%98%E8%B6%85%E8%BF%87%E4%B8%80%E5%AE%9A%E9%98%88%E5%80%BC%E4%BC%9A%E5%AF%BC%E8%87%B4%E7%B3%BB%E7%BB%9F%E5%AE%89%E5%85%A8%E6%A3%80%E6%9F%A5%E4%B8%A2%E5%8C%85"><span class="nav-number">3.3.3.</span> <span class="nav-text">frag_high_thresh, 分片的内存超过一定阈值会导致系统安全检查丢包</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%88%86%E7%89%87%E5%AE%89%E5%85%A8%E8%B7%9D%E6%A3%80%E6%9F%A5%E7%A6%BB%E4%B8%A2%E5%8C%85"><span class="nav-number">3.3.4.</span> <span class="nav-text">分片安全距检查离丢包</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%B3%BB%E7%BB%9F%E5%86%85%E5%AD%98%E4%B8%8D%E8%B6%B3%EF%BC%8C%E5%88%9B%E5%BB%BA%E6%96%B0%E5%88%86%E7%89%87%E9%98%9F%E5%88%97%E5%A4%B1%E8%B4%A5"><span class="nav-number">3.3.5.</span> <span class="nav-text">系统内存不足，创建新分片队列失败</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#MTU%E4%B8%A2%E5%8C%85"><span class="nav-number">3.3.6.</span> <span class="nav-text">MTU丢包</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#tcp%E5%B1%82%E4%B8%A2%E5%8C%85"><span class="nav-number">3.3.7.</span> <span class="nav-text">tcp层丢包</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#TIME-WAIT-%E8%BF%87%E5%A4%9A%E4%B8%A2%E5%8C%85"><span class="nav-number">3.3.7.1.</span> <span class="nav-text">TIME_WAIT 过多丢包</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%97%B6%E9%97%B4%E6%88%B3%E5%BC%82%E5%B8%B8%E4%B8%A2%E5%8C%85"><span class="nav-number">3.3.7.2.</span> <span class="nav-text">时间戳异常丢包</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#TCP%E9%98%9F%E5%88%97%E9%97%AE%E9%A2%98%E5%AF%BC%E8%87%B4%E4%B8%A2%E5%8C%85"><span class="nav-number">3.3.7.3.</span> <span class="nav-text">TCP队列问题导致丢包</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E8%BF%9E%E6%8E%A5%E6%BB%A1%E4%B8%A2%E5%8C%85"><span class="nav-number">3.3.7.4.</span> <span class="nav-text">连接满丢包</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#syn-flood%E6%94%BB%E5%87%BB%E4%B8%A2%E5%8C%85"><span class="nav-number">3.3.7.5.</span> <span class="nav-text">syn flood攻击丢包</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#PAWS%E6%9C%BA%E5%88%B6%E4%B8%A2%E5%8C%85"><span class="nav-number">3.3.7.6.</span> <span class="nav-text">PAWS机制丢包</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#TLP%E9%97%AE%E9%A2%98%E4%B8%A2%E5%8C%85"><span class="nav-number">3.3.7.7.</span> <span class="nav-text">TLP问题丢包</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%86%85%E5%AD%98%E4%B8%8D%E8%B6%B3%E5%AF%BC%E8%87%B4%E4%B8%A2%E5%8C%85"><span class="nav-number">3.3.7.8.</span> <span class="nav-text">内存不足导致丢包</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#TCP%E8%B6%85%E6%97%B6%E4%B8%A2%E5%8C%85"><span class="nav-number">3.3.7.9.</span> <span class="nav-text">TCP超时丢包</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#TCP%E4%B9%B1%E5%BA%8F%E4%B8%A2%E5%8C%85"><span class="nav-number">3.3.7.10.</span> <span class="nav-text">TCP乱序丢包</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%8B%A5%E5%A1%9E%E6%8E%A7%E5%88%B6%E4%B8%A2%E5%8C%85"><span class="nav-number">3.3.7.11.</span> <span class="nav-text">拥塞控制丢包</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#UDP%E5%B1%82%E4%B8%A2%E5%8C%85"><span class="nav-number">3.3.8.</span> <span class="nav-text">UDP层丢包</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%BA%94%E7%94%A8%E5%B1%82socket%E4%B8%A2%E5%8C%85"><span class="nav-number">3.4.</span> <span class="nav-text">应用层socket丢包</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#socket%E7%BC%93%E5%AD%98%E5%8C%BA%E6%8E%A5%E6%94%B6%E4%B8%A2%E5%8C%85"><span class="nav-number">3.4.1.</span> <span class="nav-text">socket缓存区接收丢包</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%BA%94%E7%94%A8%E8%AE%BE%E7%BD%AEtcp%E8%BF%9E%E6%8E%A5%E6%95%B0%E5%A4%A7%E5%B0%8F%E4%B8%A2%E5%8C%85"><span class="nav-number">3.4.2.</span> <span class="nav-text">应用设置tcp连接数大小丢包</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%BA%94%E7%94%A8%E5%8F%91%E9%80%81%E5%A4%AA%E5%BF%AB%E5%AF%BC%E8%87%B4%E4%B8%A2%E5%8C%85"><span class="nav-number">3.4.3.</span> <span class="nav-text">应用发送太快导致丢包</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%9B%B8%E5%85%B3%E5%B7%A5%E5%85%B7%E4%BB%8B%E7%BB%8D"><span class="nav-number">4.</span> <span class="nav-text">相关工具介绍</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="syxdevcode"
      src="/images/avatar.gif">
  <p class="site-author-name" itemprop="name">syxdevcode</p>
  <div class="site-description" itemprop="description">syxdevcode的个人博客</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">579</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">143</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">215</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/syxdevcode" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;syxdevcode" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
  </div>



      </div>
        <div class="back-to-top motion-element">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">syxdevcode</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
    <span title="站点总字数">1.9m</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="站点阅读时长">28:05</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://pisces.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  

</body>
</html>
